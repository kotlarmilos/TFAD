{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import mysql.connector\n",
    "\n",
    "# mydb = mysql.connector.connect(\n",
    "#   host=\"localhost\",\n",
    "#   user=\"root\",\n",
    "#   passwd=\"\",\n",
    "#   database=\"anomaly_detection_decision_support\"\n",
    "# )\n",
    "\n",
    "# mycursor = mydb.cursor()\n",
    "\n",
    "\n",
    "def open_connection():\n",
    "    mydb = mysql.connector.connect(\n",
    "      host=\"34.68.13.182\",\n",
    "      user=\"root\",\n",
    "      passwd=\"6g8HBIy0F8atEKtb\",\n",
    "      database=\"anomaly_detection_decision_support\"\n",
    "    )\n",
    "\n",
    "    mycursor = mydb.cursor()\n",
    "    return mydb, mycursor\n",
    "\n",
    "def close_connection(mydb, mycursor):\n",
    "    mycursor.close()\n",
    "    mydb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import mysql.connector\n",
    "\n",
    "\n",
    "def get_datasets(path):\n",
    "    datasets = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for dir in dirs:\n",
    "            with open(os.path.join(root, dir, 'metadata.json')) as json_file:\n",
    "                metadata = json.load(json_file)\n",
    "                metadata['files'] = [os.path.join(root, dir, filename) for filename in metadata['files']]\n",
    "                datasets.append(metadata)\n",
    "    return datasets\n",
    "\n",
    "def get_methods():\n",
    "    methods = [\n",
    "        {\n",
    "            'name': 'gaussian',\n",
    "            'parameters': {},\n",
    "            'isSupervised': False,\n",
    "        }, {\n",
    "            'name': 'linear_regression',\n",
    "            'parameters': {},\n",
    "            'isSupervised': False,\n",
    "        }, {\n",
    "            'name': 'pca',\n",
    "            'parameters': {},\n",
    "            'isSupervised': False,\n",
    "        }, {\n",
    "            'name': 'kmeans',\n",
    "            'parameters': {},\n",
    "            'isSupervised': False,\n",
    "        }, {\n",
    "            'name': 'neural_network',\n",
    "            'parameters': {},\n",
    "            'isSupervised': True,\n",
    "        }\n",
    "    ];\n",
    "    \n",
    "    return methods\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#initialize tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "strategy = None\n",
    "if (os.environ.get('COLAB_TPU_ADDR')!=None):\n",
    "  os.environ['TPU_ADDR'] = os.environ['COLAB_TPU_ADDR']\n",
    "# else:\n",
    "#   os.environ['TPU_ADDR'] = '10.15.20.26:8470'\n",
    "\n",
    "if (os.environ.get('TPU_ADDR')!=None):\n",
    "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['TPU_ADDR'])\n",
    "    tf.config.experimental_connect_to_cluster(resolver)\n",
    "    # This is the TPU initialization code that has to be at the beginning.\n",
    "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
    "\n",
    "def get_devices():\n",
    "    if strategy:\n",
    "        devices = ['ASIC']\n",
    "    else:\n",
    "        devices = [d.name for d in device_lib.list_local_devices() if 'XLA' not in d.name]\n",
    "    return devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# data cleansing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pymfe.mfe import MFE\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "def transform_data(dataset, rows = 1000, mfe = True):\n",
    "    # read data from files\n",
    "    df = pd.DataFrame()\n",
    "    chunk = int(rows/len(dataset['files']))\n",
    "    for filename in dataset['files']:\n",
    "        df_temp = pd.read_csv(filename, error_bad_lines=False, warn_bad_lines=False,index_col=None, nrows=chunk)\n",
    "        if df.columns.size != 0:\n",
    "            df_temp.columns=df.columns\n",
    "        df = pd.concat([df, df_temp], axis=0)\n",
    "\n",
    "    # convert objects to category codes\n",
    "    for col_name in df.columns:\n",
    "        if(df[col_name].dtype == 'object'):\n",
    "            df[col_name]= df[col_name].astype('category')\n",
    "            df[col_name] = df[col_name].cat.codes.astype('int64')\n",
    "            \n",
    "    # split dataset into features and target arrays\n",
    "    features = target = feature_scores = None\n",
    "    if (dataset['label']):\n",
    "        features = np.nan_to_num(df.drop(columns=[dataset['label']]).to_numpy())\n",
    "        target = df[dataset['label']].to_numpy()\n",
    "    else:\n",
    "        features = df.to_numpy()\n",
    "        \n",
    "    # features selection\n",
    "    if (dataset['label']):\n",
    "        clf = ExtraTreesClassifier(n_estimators=50)\n",
    "        clf = clf.fit(features, target)\n",
    "        feature_scores = pd.DataFrame(np.vstack([clf.feature_importances_]), \n",
    "                                      columns=df.drop(columns=[dataset['label']]).columns.values).to_dict(orient='records')[0]\n",
    "\n",
    "    \n",
    "    # standardize dataset\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    features = scaler.fit_transform(features)\n",
    "    features = np.nan_to_num(features)\n",
    "#     features = preprocessing.scale(features)    \n",
    "    \n",
    "    # Extract general, statistical and information-theoretic measures\n",
    "    ft = ()\n",
    "    if mfe==True:\n",
    "        mfe = MFE(groups=[\"general\", \"statistical\", \"info-theory\"],suppress_warnings = True)\n",
    "        if dataset['label']:\n",
    "            mfe.fit(features, target)\n",
    "        else:\n",
    "            mfe.fit(features)\n",
    "        ft = mfe.extract()\n",
    "        ft = np.nan_to_num(ft)\n",
    "    \n",
    "    return features, target, feature_scores, ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "\n",
    "def gaussian(features):\n",
    "    \n",
    "    # measure training time\n",
    "    start_time = tf.timestamp()\n",
    "    features = tf.constant(features)\n",
    "    mu = tf.reduce_mean(features, axis=0)\n",
    "    mu = tf.reshape(mu, [1,features.shape[1]])\n",
    "    mx = tf.matmul(tf.transpose(mu), mu)\n",
    "    vx = tf.matmul(tf.transpose(features), features)/tf.cast(tf.shape(features)[0], tf.float64)\n",
    "    sigma = vx - mx\n",
    "#     mvn = tfp.distributions.MultivariateNormalTriL(loc=mu,scale_tril=tf.linalg.cholesky(sigma))\n",
    "    train_time = tf.timestamp()-start_time\n",
    "    # measure test time\n",
    "    start_time = tf.timestamp()\n",
    "#     mvn.prob(tf.constant(features))\n",
    "    test_time = (tf.timestamp()-start_time)/features.shape[0]\n",
    "    \n",
    "    return train_time.numpy(), test_time.numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def linear(features):\n",
    "    # create linear regression\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_dim=features.shape[1], activation='linear'))\n",
    "    \n",
    "    model.compile(loss='mean_squared_error',\n",
    "                optimizer=tf.keras.optimizers.RMSprop(0.001),\n",
    "                metrics=['mean_absolute_error', 'mean_squared_error'])\n",
    "    \n",
    "    # measure training time\n",
    "    start_time = tf.timestamp()\n",
    "    model.fit(features,features)\n",
    "    train_time = tf.timestamp()-start_time\n",
    "    \n",
    "    weights = tf.transpose(model.get_weights()[0])\n",
    "    \n",
    "    # measure test time\n",
    "    start_time = tf.timestamp()\n",
    "    distance = tf.math.divide(tf.math.abs(tf.math.add(tf.reduce_sum(tf.multiply(weights, features), axis=1),model.get_weights()[1])),\n",
    "                              tf.math.sqrt(tf.reduce_sum(tf.multiply(weights, weights), axis=1)))\n",
    "    test_time = (tf.timestamp()-start_time)/features.shape[0]\n",
    "    \n",
    "    return train_time.numpy(), test_time.numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def pca(features):\n",
    "    features -= tf.reduce_mean(features, axis=0)\n",
    "    \n",
    "    start_time = tf.timestamp()\n",
    "    result = tf.linalg.svd(features)\n",
    "    train_time = test_time = (tf.timestamp()-start_time)\n",
    "    \n",
    "    return train_time.numpy(), test_time.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# function which returns data (necessary for tensorflow v1)\n",
    "def input_fn():\n",
    "    return tf.compat.v1.train.limit_epochs(tf.convert_to_tensor(features, dtype=tf.float32), num_epochs=1)\n",
    "\n",
    "\n",
    "def kmeans(features):\n",
    "    # build a model\n",
    "    model = tf.compat.v1.estimator.experimental.KMeans(5)\n",
    "    \n",
    "    #train a model\n",
    "    start_time = tf.timestamp()\n",
    "    model.train(input_fn)\n",
    "    train_time = (tf.timestamp()-start_time)\n",
    "    \n",
    "    #test a model\n",
    "    start_time = tf.timestamp()\n",
    "    model.predict_cluster_index(input_fn)\n",
    "    test_time = (tf.timestamp()-start_time)\n",
    "    \n",
    "    return train_time.numpy(), test_time.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def nn(features, target):\n",
    "    # create a NN\n",
    "    model = Sequential()\n",
    "    model.add(Dense(features.shape[1], input_dim=features.shape[1], activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # build a model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\n",
    "        tf.keras.metrics.MeanSquaredError(),\n",
    "        tf.keras.metrics.AUC(),\n",
    "        tf.keras.metrics.Precision(),\n",
    "        tf.keras.metrics.Recall(),\n",
    "        tf.keras.metrics.Accuracy()\n",
    "    ])\n",
    "    \n",
    "    # measure training time\n",
    "    start_time = tf.timestamp()\n",
    "    model.fit(features, target)\n",
    "    train_time = tf.timestamp()-start_time\n",
    "    \n",
    "    # measure test time\n",
    "    start_time = tf.timestamp()\n",
    "    results = model.predict(features)\n",
    "    test_time = (tf.timestamp()-start_time)/features.shape[0]\n",
    "    \n",
    "    # cross validation\n",
    "    validation = model.evaluate(features, target)\n",
    "    \n",
    "    return train_time.numpy(), test_time.numpy(), validation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(device, methods, dataset, features, target):\n",
    "    isSupervised = target is not None\n",
    "    for method in methods:\n",
    "        if method['isSupervised'] == isSupervised:\n",
    "            if method['name'] == 'gaussian':\n",
    "                result = gaussian(features)\n",
    "            elif method['name'] == 'linear_regression':\n",
    "                result = linear(features)\n",
    "            elif method['name'] == 'pca':\n",
    "                result = pca(features)\n",
    "            elif method['name'] == 'kmeans':\n",
    "                result = kmeans(features)\n",
    "            elif method['name'] == 'neural_network':\n",
    "                result = nn(features, target)\n",
    "                \n",
    "            print('Running',method['name'])\n",
    "            \n",
    "            insert_evaluation_info(device, method, dataset, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def clear_db():\n",
    "    mydb, mycursor = open_connection()\n",
    "    mycursor.execute('DELETE FROM algorithm')\n",
    "    mydb.commit()\n",
    "    sql = \"INSERT INTO algorithm (name, complexity) VALUES (%s, %s)\"\n",
    "    val = ('gaussian', 'O(n^2)')\n",
    "    mycursor.execute(sql, val)\n",
    "    val = ('linear_regression', 'O(n^2)')\n",
    "    mycursor.execute(sql, val)\n",
    "    val = ('pca', 'O(n^2)')\n",
    "    mycursor.execute(sql, val)\n",
    "    val = ('kmeans', 'O(n^2)')\n",
    "    mycursor.execute(sql, val)\n",
    "    val = ('neural_network', 'O(n^2)')\n",
    "    mycursor.execute(sql, val)\n",
    "    \n",
    "    mydb.commit()\n",
    "\n",
    "    mycursor.execute('DELETE FROM device_characterization')\n",
    "    mycursor.execute('DELETE FROM device')\n",
    "\n",
    "    sql = \"INSERT INTO device (name, type) VALUES (%s, %s)\"\n",
    "    val = ('Intel Xeon', 'CPU')\n",
    "    mycursor.execute(sql, val)\n",
    "    \n",
    "    device_id = mycursor.lastrowid\n",
    "    sql = \"INSERT INTO device_characterization (device_id, name, value) VALUES (%s, %s, %s)\"\n",
    "    val = (device_id, 'transistor_count', '7.2')\n",
    "    mycursor.execute(sql, val)\n",
    "    val = (device_id, 'core_count', '2')\n",
    "    mycursor.execute(sql, val)\n",
    "    val = (device_id, 'technology', '22')\n",
    "    mycursor.execute(sql, val)\n",
    "    val = (device_id, 'power_dissipation', '180')\n",
    "    mycursor.execute(sql, val)\n",
    "    val = (device_id, 'flops', '90')\n",
    "    mycursor.execute(sql, val)\n",
    "    val = (device_id, 'fequency', '4300')\n",
    "    mycursor.execute(sql, val)\n",
    "    val = (device_id, 'memory_type', 'DRAM')\n",
    "    mycursor.execute(sql, val)\n",
    "    val = (device_id, 'memory_size', '13')\n",
    "    mycursor.execute(sql, val)\n",
    "    val = (device_id, 'memory_bandwidth', '')\n",
    "    mycursor.execute(sql, val)\n",
    "    val = (device_id, 'weight', '')\n",
    "    mycursor.execute(sql, val)\n",
    "\n",
    "    sql = \"INSERT INTO device (name, type) VALUES (%s, %s)\"\n",
    "    val = ('Tesla K80', 'GPU')\n",
    "    mycursor.execute(sql, val)\n",
    "    device_id = mycursor.lastrowid\n",
    "    sql = \"INSERT INTO device_characterization (device_id, name, value) VALUES (%s, %s, %s)\"\n",
    "    val = (device_id, 'transistor_count', '7.1')\n",
    "    mycursor.execute(sql, val)\n",
    "    val = (device_id, 'core_count', '2496')\n",
    "    mycursor.execute(sql, val)\n",
    "    val = (device_id, 'technology', '28')\n",
    "    mycursor.execute(sql, val)\n",
    "    val = (device_id, 'power_dissipation', '300')\n",
    "    mycursor.execute(sql, val)\n",
    "    val = (device_id, 'flops', '2910')\n",
    "    mycursor.execute(sql, val)\n",
    "    val = (device_id, 'fequency', '1562')\n",
    "    mycursor.execute(sql, val)\n",
    "    val = (device_id, 'memory_type', 'DRAM')\n",
    "    mycursor.execute(sql, val)\n",
    "    val = (device_id, 'memory_size', '12')\n",
    "    mycursor.execute(sql, val)\n",
    "    val = (device_id, 'memory_bandwidth', '')\n",
    "    mycursor.execute(sql, val)\n",
    "    val = (device_id, 'weight', '')\n",
    "    mycursor.execute(sql, val)\n",
    "\n",
    "    sql = \"INSERT INTO device (name, type) VALUES (%s, %s)\"\n",
    "    val = ('Google TPU', 'ASIC')\n",
    "    mycursor.execute(sql, val)\n",
    "    device_id = mycursor.lastrowid\n",
    "    sql = \"INSERT INTO device_characterization (device_id, name, value) VALUES (%s, %s, %s)\"\n",
    "    val = (device_id, 'transistor_count', '2.1')\n",
    "    mycursor.execute(sql, val)\n",
    "    val = (device_id, 'core_count', '2496')\n",
    "    mycursor.execute(sql, val)\n",
    "    val = (device_id, 'technology', '28')\n",
    "    mycursor.execute(sql, val)\n",
    "    val = (device_id, 'power_dissipation', '40')\n",
    "    mycursor.execute(sql, val)\n",
    "    val = (device_id, 'flops', '180000')\n",
    "    mycursor.execute(sql, val)\n",
    "    val = (device_id, 'fequency', '700')\n",
    "    mycursor.execute(sql, val)\n",
    "    val = (device_id, 'memory_type', 'SRAM')\n",
    "    mycursor.execute(sql, val)\n",
    "    val = (device_id, 'memory_size', '16')\n",
    "    mycursor.execute(sql, val)\n",
    "    val = (device_id, 'memory_bandwidth', '')\n",
    "    mycursor.execute(sql, val)\n",
    "    val = (device_id, 'weight', '')\n",
    "    mycursor.execute(sql, val)\n",
    "    mydb.commit()\n",
    "\n",
    "    mycursor.execute('DELETE FROM dataset_characterization')\n",
    "    mycursor.execute('DELETE FROM feature_score')\n",
    "    mycursor.execute('DELETE FROM dataset')\n",
    "    \n",
    "    \n",
    "    mycursor.execute('DELETE FROM performance')\n",
    "    mycursor.execute('DELETE FROM parameter')\n",
    "    mycursor.execute('DELETE FROM evaluation')\n",
    "    \n",
    "    mydb.commit()\n",
    "    \n",
    "    close_connection(mydb, mycursor)\n",
    "    \n",
    "def insert_data_info(dataset, feature_score, ft):\n",
    "    mydb, mycursor = open_connection()\n",
    "    mycursor.execute(\"SELECT * FROM dataset WHERE name='\"+str(dataset['name'])+\"'\")\n",
    "    row = mycursor.fetchone()\n",
    "    if row:\n",
    "        return row[0]\n",
    "    \n",
    "    sql = \"INSERT INTO dataset (name, type_of_data, domain, anomaly_types, anomaly_space, anomaly_entropy, label, files) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "    val = (dataset['name'], str(dataset['type_of_data']).strip('[]'), str(dataset['domain']).strip('[]'), str(dataset['anomaly_types']).strip('[]'), dataset['anomaly_space'], dataset['anomaly_entropy'], dataset['label'], str(dataset['files']).strip('[]'))\n",
    "    mycursor.execute(sql, val)\n",
    "    mydb.commit()\n",
    "    dataset['id'] = mycursor.lastrowid;\n",
    "    \n",
    "    if len(ft) == 2:\n",
    "        for i in range(len(ft[0])):\n",
    "            sql = \"INSERT INTO dataset_characterization (dataset_id, name, value) VALUES (%s, %s, %s)\"\n",
    "            val = (dataset['id'], str(ft[0][i]), str(ft[1][i]))\n",
    "            mycursor.execute(sql, val)\n",
    "        mydb.commit()\n",
    "        \n",
    "    if feature_score:\n",
    "        for key in feature_score:\n",
    "            sql = \"INSERT INTO feature_score (dataset_id, name, value) VALUES (%s, %s, %s)\"\n",
    "            val = (dataset['id'], str(key), str(feature_score[key]))\n",
    "            mycursor.execute(sql, val)\n",
    "        mydb.commit()\n",
    "        \n",
    "    close_connection(mydb, mycursor)\n",
    "    return dataset['id']\n",
    "\n",
    "\n",
    "def insert_evaluation_info(device, method, dataset, result):\n",
    "    device_type = 'TPU'\n",
    "    if 'CPU' in device:\n",
    "        device_type = 'CPU'\n",
    "    elif 'GPU' in device:\n",
    "        device_type = 'GPU'\n",
    "        \n",
    "    mydb, mycursor = open_connection()\n",
    "    mycursor.execute(\"SELECT * FROM device WHERE type='\"+device_type+\"'\")\n",
    "    device_id = mycursor.fetchone()[0]\n",
    "    mycursor.execute(\"SELECT * FROM dataset WHERE id=\"+str(dataset['id']))\n",
    "    dataset_id = mycursor.fetchone()[0]\n",
    "    mycursor.execute(\"SELECT * FROM algorithm WHERE name='\"+method['name']+\"'\")\n",
    "    algorithm_id = mycursor.fetchone()[0]\n",
    "\n",
    "    sql = \"INSERT INTO evaluation (evaluation_id, dataset_id, algorithm_id, device_id, training_time, inference_time) VALUES (%s, %s, %s, %s, %s, %s)\"\n",
    "    val = (1, dataset_id, algorithm_id, device_id, str(result[0]), str(result[1]))\n",
    "    mycursor.execute(sql, val)\n",
    "    mydb.commit()\n",
    "\n",
    "    if len(result) == 3:\n",
    "        evaluation_id = mycursor.lastrowid;\n",
    "        sql = \"INSERT INTO performance (evaluation_id, name, value) VALUES (%s, %s, %s)\"\n",
    "        val = (evaluation_id, 'mean_squared_error', str(result[2][0]))\n",
    "        mycursor.execute(sql, val)\n",
    "        val = (evaluation_id, 'auc', str(result[2][0]))\n",
    "        mycursor.execute(sql, val)\n",
    "        val = (evaluation_id, 'precision', str(result[2][1]))\n",
    "        mycursor.execute(sql, val)\n",
    "        val = (evaluation_id, 'recall', str(result[2][2]))\n",
    "        mycursor.execute(sql, val)\n",
    "        val = (evaluation_id, 'accuracy', str(result[2][3]))\n",
    "        mycursor.execute(sql, val)\n",
    "        mydb.commit()\n",
    "        \n",
    "    close_connection(mydb, mycursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Datasets for evaluation: 15 ***\n",
      "*** Dataset: 1 Name: Green Card & H1B (2014-2018)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Warning: It is not possible make equal discretization\n",
      " * Warning: Can't summarize feature 'g_mean' with summary 'sd'. Will set it as 'np.nan'.\n",
      " * Warning: Can't summarize feature 'g_mean' with summary 'mean'. Will set it as 'np.nan'.\n",
      " * Warning: Can't summarize feature 'h_mean' with summary 'sd'. Will set it as 'np.nan'.\n",
      " * Warning: Can't summarize feature 'h_mean' with summary 'mean'. Will set it as 'np.nan'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: (100, 27)\n",
      "Running gaussian\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 3.0896 - mean_absolute_error: 1.3694 - mean_squared_error: 3.0896\n",
      "Running linear_regression\n",
      "Running pca\n",
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/gv/w4dx2qm514jdz__wrb8hzjc80000gn/T/tmpv_89lwlg\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/gv/w4dx2qm514jdz__wrb8hzjc80000gn/T/tmpv_89lwlg', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:From /Users/miloskotlar/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /Users/miloskotlar/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:From <ipython-input-8-90292c9b9818>:3: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /Users/miloskotlar/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/input.py:112: BaseResourceVariable.count_up_to (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Dataset.range instead.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /var/folders/gv/w4dx2qm514jdz__wrb8hzjc80000gn/T/tmpv_89lwlg/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1...\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/gv/w4dx2qm514jdz__wrb8hzjc80000gn/T/tmpv_89lwlg/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1...\n",
      "WARNING:tensorflow:Training with estimator made no steps. Perhaps input is empty or misspecified.\n",
      "INFO:tensorflow:Loss for final step: None.\n",
      "Running kmeans\n",
      "*** Dataset: 2 Name: Software Operational Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Warning: It is not possible make equal discretization\n",
      " * Warning: invalid value encountered in true_divide\n",
      " * Warning: invalid value encountered in true_divide\n",
      " * Warning: Can't summarize feature 'freq_class' with summary 'sd'. Will set it as 'np.nan'.\n",
      " * Warning: Can't summarize feature 'can_cor' with summary 'sd'. Will set it as 'np.nan'.\n",
      " * Warning: Mean of empty slice.\n",
      " * Warning: invalid value encountered in true_divide\n",
      " * Warning: Can't summarize feature 'can_cor' with summary 'mean'. Will set it as 'np.nan'.\n",
      " * Warning: Can't summarize feature 'cor' with summary 'sd'. Will set it as 'np.nan'.\n",
      " * Warning: Can't summarize feature 'cor' with summary 'mean'. Will set it as 'np.nan'.\n",
      " * Warning: Casting complex values to real discards the imaginary part\n",
      " * Warning: Can't summarize feature 'g_mean' with summary 'sd'. Will set it as 'np.nan'.\n",
      " * Warning: Can't summarize feature 'g_mean' with summary 'mean'. Will set it as 'np.nan'.\n",
      " * Warning: Can't extract feature 'gravity'.\n",
      " Exception message: ValueError('attempt to get argmin of an empty sequence').\n",
      " Will set it as 'np.nan' for all summary functions.\n",
      " * Warning: Can't summarize feature 'h_mean' with summary 'sd'. Will set it as 'np.nan'.\n",
      " * Warning: Can't summarize feature 'h_mean' with summary 'mean'. Will set it as 'np.nan'.\n",
      " * Warning: invalid value encountered in greater_equal\n",
      " * Warning: Input data for shapiro has range zero. The results may not be accurate.\n",
      " * Warning: Can't extract feature 'sd_ratio'.\n",
      " Exception message: ZeroDivisionError('float division by zero').\n",
      " Will set it as 'np.nan' for all summary functions.\n"
     ]
    }
   ],
   "source": [
    "datasets = get_datasets('../GoogleDrive/Academic/PhD/III/datasets/')\n",
    "devices = get_devices()\n",
    "methods = get_methods()\n",
    "\n",
    "# tf.profiler.experimental.start('logdir')\n",
    "\n",
    "print('*** Datasets for evaluation:', len(datasets),'***')\n",
    "i = 1\n",
    "for dataset in datasets:\n",
    "    print('*** Dataset:',i,'Name:',dataset['name'])\n",
    "    i+=1\n",
    "    features, target, feature_score, ft = transform_data(dataset, 100, True)\n",
    "    dataset['id'] = insert_data_info(dataset, feature_score, ft)\n",
    "    print('Features:', features.shape)\n",
    "    \n",
    "    for device in devices:\n",
    "        if device == 'ASIC':\n",
    "            with strategy.scope():\n",
    "                evaluate(device, methods, dataset, features, target)\n",
    "        else:\n",
    "            with tf.device(device):\n",
    "                evaluate(device, methods, dataset, features, target)\n",
    "        \n",
    "print('*** DONE ***')\n",
    "\n",
    "# tf.profiler.experimental.stop()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir=./logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
